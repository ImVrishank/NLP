{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOsYTKkIIkFf9XU95ww7X4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImVrishank/NLP/blob/main/GPT-2/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWyE8DwG0bU0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # number of sequences we will run in parallel. Works as B in logits\n",
        "block_size = 256 # max context length. Works as T in the logits\n",
        "max_iters = 5000 # number of times we will run across the training dataset\n",
        "eval_interval = 500 # this is used in the function estimate_loss. every eval_interval, we will evaluate the loss on the train and val sets\n",
        "learning_rate = 3e-4 # learning rate for the optimizer\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available\n",
        "eval_iters = 200 # it prints the loss of the train and val sets every eval_iters number of batches. This drastically reduces noise. It averages out the loss of all the batches passed and then gives it out as the loss of the set.\n",
        "n_embd = 384 # works as the C in the logits. It is the number of channels in the input data. More channels means more features to learn from.\n",
        "n_head = 6 # number of heads we will compute parallelly in the MultiHeadAttention. Explain in detail in the blog.\n",
        "n_layer = 6 # the number of layers we have in the transformer, written when cleaning up the code.\n",
        "dropout = 0.2 # the percentage of neuron we will drop in the dropout layer. It is used to prevent overfitting. Explained in detail in the blog, under the Cleaning up the code section's Dropout.\n",
        "# ------------"
      ],
      "metadata": {
        "id": "oL2fs1pS0chU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YmnE-6zN0e4A",
        "outputId": "3a730dac-b872-4d1f-b9f0-b5badfe6c45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_H3_3de00zP",
        "outputId": "2bf86114-2f83-47d2-e272-4db7a9615adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-14 13:34:06--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2025-03-14 13:34:06 (170 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text))) # sorted list of all unique characters, we will use this as tokens as this is a character based model\n",
        "vocab_size = len(chars) # number of unique tokens\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # mapping from character to integer\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # mapping from integer to character\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: a function that takes in a string, outputs a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: function that takes in a list of integers, outputs a string\n"
      ],
      "metadata": {
        "id": "SFuFqy_o3MAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long) # encoding the whole text into integers using the encode function\n",
        "n = int(0.9*len(data)) # split fraction. 90% of the data will be training data and the rest 10% is the validation data\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# This function is used to make batches of features and labels. It takes in the split as an argument. This is explained in detail in the blog and the colab file.\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data # what kinda data are we making batches of? train or val\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # we are generating batch_size different numbers which will be the index of the first element of the batch_size number of batches we are going to hand to the GPU.\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # stacking each of the batches' features over each other, using the indexes that we generated earlier\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # stackinge each of the batches' labels over each other, using the indexes that we generated earlier corresponding to the features.\n",
        "    x, y = x.to(device), y.to(device) # converting the features and labels to the device (GPU or CPU)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad() # telling Pytorch that we are not going to be updating the weights in this function. This is used in the function estimate_loss. This betters memory usage and computation time.\n",
        "\n",
        "# This function is used to estimate the loss of the model on the train and val sets. It is used to evaluate the model's performance on the train and val sets.\n",
        "# We will only estimate the loss once every every eval_iters batches have passed. This is done to reduce noise in the loss.\n",
        "# We will average out the loss of all the batches passed and then give it out as the loss of the set.\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # setting the model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split) # getting the features and labels of the batch\n",
        "            logits, loss = model(X, Y) # getting the logits and loss of the model\n",
        "            losses[k] = loss.item() # storing the loss in the losses tensor\n",
        "        out[split] = losses.mean() # averaging out the loss of all the batches we have calculated above.\n",
        "    model.train() # setting the model back to training mode\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # explained in great detail in the blog\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) # explained in great detail in the blog\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False) # explained in great detail in the blog\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # register_buffer ensures it is not affected by optimizer.step()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) # explained in cleaning up of code (dropout) of the blog.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input x of size (batch, time-step, channels) ---> (B, T, C) defined in hyperparameters\n",
        "        # output of size (batch, time-step, head size) ---> (B, T, head_size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,head_size)\n",
        "        q = self.query(x) # (B,T,head_size)\n",
        "        # self attention: compute the weight matrix. Explained in great detail in the blog.\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei) # more dropuout, explained in the blog\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs) also explained in the blog\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) ---> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" As per documentation, this is basically running multiple heads of SelfAttention parallelly. Then we concatenate the heads at the end.  \"\"\"\n",
        "    # Here we are processing 256 tokens per block, and we are running 6 heads in parallel. so each head will process 256/6 = 42 tokens. and all 6 of them will be concatenated in the end.\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # defining a list of all layers of the neural network. We use this instead of a python list because pytorch can analize this in real time unlike in a python list.\n",
        "        # This list now contains num_heads number of items. where for each cell we call the Head class and pass it head_size for its constructor.\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # residual connections. Explained in the blog under the Residual connections section.\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd) # concatenating all the heads back together.\n",
        "        self.dropout = nn.Dropout(dropout) # dropout again, explained in the blog.\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatenating.\n",
        "        out = self.dropout(self.proj(out)) # projecting it then passing it through a dropout layer.\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" Explained in great detain the blog. I have explained why we try to make the model non linear and how we go about doing it here. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" This is basically an amalgamation of all The entire transformer architecture. Here we are basically connecting all the dots and connections for all the other parts of the architecture that we have worked on till now.  \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # residual connections by using x = x + (deviated value)\n",
        "        \"\"\"LayerNorm by altering deviated value by (self.sa(self.ln1(x))) before the Self attention and altering deviated value by (self.ffwd(ln2(x))) before the FeedForward sections.\n",
        "        Not similar to the architecture diagram in blog as we are implemeting LayerNorm before FeedForward as in case of ln1 and before Self Attention in case of ln2. \"\"\"\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    \"\"\" This is the final model that we will be using to train and generate text. used to train and generate text. used to train and generate text. \"\"\"\n",
        "    # all the details can be found in the blog.\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from the token embedding table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # interspacing communication multiple times.\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # language model head\n",
        "\n",
        "        # for reporducibility\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    # this block doesnt do anything important. It is so that building on this code is easier for later on\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "V2pzyQjf3Qbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device) # moving the model to the device (GPU or CPU)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# We are using AdamW for this model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# this is used to reduce noise. It averages out the loss of all the batches passed and then gives it out as the loss of the set.\n",
        "# We will only estimate the loss once every every eval_iters batches have passed. This is done to reduce noise in the loss.\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "# uncomment the following line to save the generated text to a file of 10000 tokens.\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSCQDR7S3VPs",
        "outputId": "01d0ec92-6d92-4b71-ed4e-16d4ece16258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2452, val loss 4.2423\n",
            "step 500: train loss 1.7546, val loss 1.8979\n",
            "step 1000: train loss 1.4079, val loss 1.6385\n",
            "step 1500: train loss 1.2773, val loss 1.5380\n",
            "step 2000: train loss 1.1938, val loss 1.5112\n",
            "step 2500: train loss 1.1337, val loss 1.4907\n",
            "step 3000: train loss 1.0770, val loss 1.4892\n",
            "step 3500: train loss 1.0198, val loss 1.4932\n",
            "step 4000: train loss 0.9686, val loss 1.5025\n",
            "step 4500: train loss 0.9163, val loss 1.5405\n",
            "step 4999: train loss 0.8618, val loss 1.5612\n",
            "\n",
            "Even Angelo brother:\n",
            "Not shake my words. Death, you mourn this:\n",
            "Let us wake that 'far and from on mine own\n",
            "That I miscarelean to list. Mine wonten wife\n",
            "Might wash my lord, counted 'good no sin't, I'll plant\n",
            "My issue on my sufit I may live to love,\n",
            "When done but nothing all, that theink I lost done;\n",
            "Of you near, since we do pert we pire eyes\n",
            "Upon the rest of journess.\n",
            "\n",
            "SICINIUS:\n",
            "How\n",
            "must the Too Boingbroke have to do wait their guilty:\n",
            "If you love himself by me to their hands;\n",
            "If, to then I'ld to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = decode(m.generate(context, max_new_tokens=10000)[0].tolist())\n",
        "\n",
        "\n",
        "with open(\"output.txt\", \"w\") as file:\n",
        "    file.write(output_text)"
      ],
      "metadata": {
        "id": "DUuCO5soFLZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"output.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "y2NuO143FX7M",
        "outputId": "8b77981b-1756-4e25-cb40-5c26669cfad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c487c8e4-d3fe-4b7a-92bd-553e5f3da197\", \"output.txt\", 10001)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"model.pth\")"
      ],
      "metadata": {
        "id": "c7TEhdmfKDPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FE5ADb59KHN-",
        "outputId": "25553a83-1909-41a2-c00c-63494fe8fd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_12abc921-7e42-44e0-a64a-db6bf8ff34c7\", \"model.pth\", 52700770)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to reuse the model that you saved\n",
        "\n",
        "model = torch.load(\"model.pth\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "2DngILvlKU-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}