# ðŸ§  NLP

**NLP** is where Iâ€™m diving deep into natural language processing by building popular models from scratch â€” transformers, attention mechanisms, tokenizers â€” all from first principles.

---

## ðŸ§ª Whatâ€™s in here so far

### â–¸ GPT-2 (Tiny Shakespeare)
- Character-level transformer trained on the Tiny Shakespeare dataset  
- Implements multi-head self-attention, GELU, layer norm, and weight tying  
- Fully autoregressive generation with temperature sampling  
- Built from scratch with a clear focus on *how* it works

ðŸ‘‰ [Project page]([https://your-notion-blog-link.com](https://vrishank.super.site/gpt-2)) 

---

## ðŸ”­ Whatâ€™s next

Planned models (also from scratch):
- BERT â€” encoder with MLM + NSP
- LLaMA â€” lightweight LLM architecture
- Tokenizers â€” BPE and WordPiece
- Pretraining + fine-tuning pipelines

---

## ðŸ’­ Why Iâ€™m building this

Iâ€™ve always been curious about *how* these models actually work â€” not just what buttons to press to make them run.  
This repo is my way of pulling apart the black boxes and seeing the moving pieces inside, one model at a time.

---

Made with lots of love, patience, and way too many print statements.
